<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Experiments on a Delta Table</title>
    <link rel="stylesheet" href="styles.css" />
    <link href="prism.css" rel="stylesheet" />
</head>
<body>

<div class="container">
    <header>
        <h1>Experiments on a Delta table</h1>
    </header>

    <main>

    <p>
        To learn how delta tables work, I'm going to do some operations on a Delta Table and observe the changes, ie. poke around. We'll start by creating a Delta Table 
        the <a href="https://github.com/allisonhorst/palmerpenguins/tree/main">Palmer Penguins</a> dataset. The dataset describes.. well, penguins. 
	</p>
    <h3>Getting started</h3>
    <p>
        To run the examples below, you'll need <a href="https://github.com/duckdb/duckdb">DuckDB</a> and the <a href="https://pypi.org/project/deltalake/">deltalake</a> python package.
    </p>
    
    <p>
        You can find the preceding code with all the boring stuff <a href="article2-appendix.html">here</a>. I'll only show the operations on the delta table as that is what we are here for. 
        (Although if you are not familiar with DuckDB, you should check it out. SQL-queries on CSV-files over the internet!)
    </p>
    <h3>Experiments start here</h3>


    <p>
        We'll start with only option you really have - creating a delta table. Here goes nothing.
    </p>

<pre>
<code class="language-python">
    df = duckdb.sql(f"SELECT * FROM {penguins_file}")
    write_deltalake(delta_table, df)
    log("Created delta table")    
</code>
</pre>
<p>
    In the code <code class="language-python">penguins</code> is merely the path of the delta table and  <code class="language-python">penguins_file</code> the path to the csv-file with the data.
</p>


    
    <p>The <code class="language-python">log()</code> function is a custom function that logs the name of the operation and contents of the delta table. The parquet files are are printed in the order they were created in.
        After the first operation we have the following parquet-files (the filenames vary on each iteration, so don't mind them) on disk. The delta log files are not printed although they are of interest.
    </p>

    
    <pre class="log">
    <code >
    --- Created delta table ---
    delta_penguins/part-00001-3f349227-ae42-4d19-8eba-6165f0c4a130-c000.snappy.parquet, size: 5.5 Kb
    </code>
    </pre>

    <p>
        So, okay, the penguins data is stored in a single parquet file. Let's remove some of the penguins and see what happens then?
    </p>

<pre>
<code class="language-python">
    dt = DeltaTable(delta_table)
    dt.delete("species = 'Adelie'")
    log("After deleting Adelie")    
</code>
</pre>
    
    <p>The delta table now has these files</p>
    <pre class="log">
    <code >
    --- After deleting Adelie ---
    delta_penguins/part-00001-3f349227-ae42-4d19-8eba-6165f0c4a130-c000.snappy.parquet, size: 5.5 Kb
    delta_penguins/part-00001-b641eec0-1a7a-4c27-a5c0-d17b110fd9ac-c000.snappy.parquet, size: 4.4 Kb
    </code>
    </pre>
    <p>
        But wait - deleting some of the penguin data almost doubles our storage consumption from 5.5 Kb to 9.9 Kb!
    </p><p>What happens is that instead of removing any actual data, 
        delta table marks the first parquet file as removed. So if you'd query the table, it'd only read the latter parquet file. This information is stored 
        in the logs like this:
    </p>
<pre>
<code class="language-json">
    {"add":{"path":"part-00001-b641eec0-1a7a-4c27-a5c0-d17b110fd9ac-c000.snappy.parquet","partitionValues":{},"size":4532,"modificationTime":1744742038876,"dataChange":true,"stats":"{\"numRecords\":192,\"minValues\":{\"flipper_length_mm\":\"178\",\"body_mass_g\":\"2700\",\"bill_length_mm\":\"40.9\",\"bill_depth_mm\":\"13.1\",\"sex\":\"NA\",\"species\":\"Chinstrap\",\"island\":\"Biscoe\",\"year\":2007},\"maxValues\":{\"year\":2009,\"bill_length_mm\":\"NA\",\"body_mass_g\":\"NA\",\"flipper_length_mm\":\"NA\",\"island\":\"Dream\",\"sex\":\"male\",\"species\":\"Gentoo\",\"bill_depth_mm\":\"NA\"},\"nullCount\":{\"bill_length_mm\":0,\"year\":0,\"island\":0,\"sex\":0,\"bill_depth_mm\":0,\"body_mass_g\":0,\"species\":0,\"flipper_length_mm\":0}}","tags":null,"deletionVector":null,"baseRowId":null,"defaultRowCommitVersion":null,"clusteringProvider":null}}
    {"remove":{"path":"part-00001-3f349227-ae42-4d19-8eba-6165f0c4a130-c000.snappy.parquet","dataChange":true,"deletionTimestamp":1744742038876,"extendedFileMetadata":true,"partitionValues":{},"size":5637}}
    {"commitInfo":{"timestamp":1744742038876,"operation":"DELETE","operationParameters":{"predicate":"species = 'Adelie'"},"operationMetrics":{"execution_time_ms":6,"num_added_files":1,"num_copied_rows":192,"num_deleted_rows":152,"num_removed_files":1,"rewrite_time_ms":4,"scan_time_ms":2},"clientVersion":"delta-rs.py-0.25.5","readVersion":0}}
</code>
</pre>
    <p>
        The log is a hefty piece of information, but the gist is easily observed. One parquet file is added and another removed.
    </p>
<h3>Delete a penguin, add a penguin</h3>    
    <p>
        Showing some remorse, we'd like to have Adelie-penguins in our dataset. So let's do an insert to the table. 
    </p>

<pre><code class="language-python">
    new_penguins = pa.table(
        [
            pa.array(['Adelie','Adelie']),
            pa.array(['Torgersen','Torgersen']),
            pa.array([123.4,999]),
            pa.array([12,13]),
            pa.array([190,98]),
            pa.array([3750,3750]),
            pa.array(['male','female']),
            pa.array([2007,1995]),
        ],
        names=[
            "species","island","bill_length_mm","bill_depth_mm",
            "flipper_length_mm","body_mass_g","sex","year"
        ]
    )

    write_deltalake(delta_table, new_penguins, mode="append")
    log("After inserting Adelies 1")

    write_deltalake(delta_table, new_penguins, mode="append")
    log("After inserting Adelies 2")
</code></pre>

    <p>We see another file pop up!</p>
    <pre class="log">
    <code >
    --- After inserting Adelies 1 ---
    delta_penguins/part-00001-3f349227-ae42-4d19-8eba-6165f0c4a130-c000.snappy.parquet, size: 5.5 Kb
    delta_penguins/part-00001-b641eec0-1a7a-4c27-a5c0-d17b110fd9ac-c000.snappy.parquet, size: 4.4 Kb
    delta_penguins/part-00001-5298e7f8-a67a-45ce-8c0c-b7ca228d7b2d-c000.snappy.parquet, size: 2.5 Kb
    </code>
    </pre>
    <p>It's not enough, insert them again!</p>
    <pre class="log">
    <code >
    --- After inserting Adelies 2 ---
    delta_penguins/part-00001-3f349227-ae42-4d19-8eba-6165f0c4a130-c000.snappy.parquet, size: 5.5 Kb
    delta_penguins/part-00001-b641eec0-1a7a-4c27-a5c0-d17b110fd9ac-c000.snappy.parquet, size: 4.4 Kb
    delta_penguins/part-00001-5298e7f8-a67a-45ce-8c0c-b7ca228d7b2d-c000.snappy.parquet, size: 2.5 Kb
    delta_penguins/part-00001-78cf7a57-a7fa-4a34-a432-dbc36f8f73a9-c000.snappy.parquet, size: 2.5 Kb        
    </code>
    </pre>
    <p>
        Hmm. Each insert created a new parquet-file. When you think about it, it does make sense. If you append to a table, there's no need to touch any existing data, is there?
        Now, if you'd query the table at this point, the query engine would* read all of three parquet files. So really the active data in our table is stored in three separate files.
    </p>
    <p>
    Showing a different kind of remorse, we decide to delete Adelie-penguins we just inserted. 
    </p>
<pre><code class="language-python">
    dt = DeltaTable(delta_table)
    dt.delete("species = 'Adelie'")
    log("After deleting Adelie again")
</code>
</pre>
    <p>
        We observe a pequliar phenomenon - nothing happens! There's still four parquet-files and their filesizes have not changed.
        If it's not entirely clear to you why nothing needs to change, I urge you to think for a while before reading on.

    </p>
<pre class="log">
<code >
--- After deleting Adelie again ---
delta_penguins/part-00001-3f349227-ae42-4d19-8eba-6165f0c4a130-c000.snappy.parquet, size: 5.5 Kb
delta_penguins/part-00001-b641eec0-1a7a-4c27-a5c0-d17b110fd9ac-c000.snappy.parquet, size: 4.4 Kb
delta_penguins/part-00001-5298e7f8-a67a-45ce-8c0c-b7ca228d7b2d-c000.snappy.parquet, size: 2.5 Kb
delta_penguins/part-00001-78cf7a57-a7fa-4a34-a432-dbc36f8f73a9-c000.snappy.parquet, size: 2.5 Kb    
</code>
</pre>
    <p>
        We know that the records for Adelie-penguins exists only in the last two parquet-files as they were created when we inserted said penguins. So when you think about, 
        all you really need to do to get rid of Adelie-penguins is to mark those files as removed in the logs. 
    </p>
    <p>
        This differs from the first delete we did in that this time we had parquet-files that had <i>only</i> Adelie-penguins. If we'd insert another type of penguin 
        with an Adelie-penguin, the query would have had to create new version of the file with Adelie-penguins removed. This is what happened with the first delete.
    </p>
    <p>
        End of transmission.
    </p>
    </main>
    <p class="footnote">*It's a lie. It <i>might</i> read all three tables, but it might not. We'll get to that later (maybe?).</p>    
</div>

<script src="prism.js"></script>
</body>
</html>
